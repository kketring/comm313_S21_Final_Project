{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(tfile):\n",
    "    \n",
    "    tweets = []\n",
    "    for line in open(tfile):\n",
    "        try:\n",
    "            tweets.append(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    toks = tt.tokenize(tweet['text'])\n",
    "    tweet['tokens'] = toks\n",
    "    \n",
    "    \n",
    "    tweet['Valence']=0\n",
    "    tweet['Dominance']=0\n",
    "    tweet['Arousal']=0\n",
    "    \n",
    "    tweet['VAD_toks']=[]\n",
    "    \n",
    "    for t in toks:\n",
    "        if t.lower() in NRC_VAD.keys():\n",
    "            scores = NRC_VAD[t.lower()]\n",
    "            scores['tok']=t\n",
    "            \n",
    "            tweet['Valence']+=scores['V']\n",
    "            tweet['Arousal']+=scores['A']\n",
    "            tweet['Dominance']+=scores['D']\n",
    "            \n",
    "            tweet['VAD_toks'].append(scores)\n",
    "    \n",
    "    \n",
    "    for dimension in ('Valence','Arousal','Dominance'):\n",
    "        if len(tweet['VAD_toks'])>0:\n",
    "            tweet[dimension] /= len(tweet['VAD_toks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_query_tweets(query, date_since, date_until, max=1000):\n",
    "    print(f\"Downloading tweets for query: '{query}' from {date_since} to {date_until} (max of {max})\")\n",
    "\n",
    "    tweet_list = []\n",
    "    qu = query\n",
    "    query = f'{query} since:{date_since} until:{date_until}'\n",
    "    \n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i>=max:\n",
    "            break\n",
    "    \n",
    "\n",
    "        tweet_dict = {\n",
    "            'id': tweet.id,\n",
    "            'created_at': tweet.date.strftime('%Y-%m-%d %H:%M'),\n",
    "            'text': tweet.content,\n",
    "            'username': tweet.username,\n",
    "            'query':qu\n",
    "        }\n",
    "\n",
    "        tweet_list.append(tweet_dict)\n",
    "        \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranged_query(date_range,query,out_path,max=1000):\n",
    "    \"\"\"\n",
    "    takes start and end dates and collects tweets \n",
    "    for each query. Saves results as json grouped by year\n",
    "    \n",
    "    Args:\n",
    "        - date_range: tuple containing start and end '%Y-%m-%d'\n",
    "        - query: list containing searchable terms\n",
    "        - out_path: dir to save json files\n",
    "        - max: maximum number of tweets to search\n",
    "    \n",
    "    Returns:\n",
    "        None: saves to disc\n",
    "    \"\"\"\n",
    "    \n",
    "    since, until = date_range # a tuple of start and end date (%Y-%m-%d)\n",
    "    all_tweets = []\n",
    "    for qu in query: # for each query download tweets within range and add to larger list\n",
    "        tweet_list = download_query_tweets(qu,since,until,max=max)\n",
    "        all_tweets.extend(tweet_list)\n",
    "    \n",
    "    year = since.split('-')[0]  # get what year the query is for\n",
    "    \n",
    "    # create a folder for the year if it doesn't exist already\n",
    "    if not os.path.exists(f\"{out_path}/{year}\"):\n",
    "        os.makedirs(f\"{out_path}/{year}\")\n",
    "    \n",
    "    # save json\n",
    "    out_file_name = f\"{out_path}/{year}/{since}_{until}_{'_'.join(query)}.json\"\n",
    "    with open(out_file_name,'w') as out_file:\n",
    "        out_file.write(json.dumps(all_tweets))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '#ncaa' from 2020-04-01 to 2020-04-11 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2020-04-01 to 2020-04-11 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2020-04-01 to 2020-04-11 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2020-09-12 to 2020-09-23 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2020-09-12 to 2020-09-23 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2020-09-12 to 2020-09-23 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2019-04-01 to 2019-04-11 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2019-04-01 to 2019-04-11 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2019-04-01 to 2019-04-11 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2019-09-12 to 2019-09-23 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2019-09-12 to 2019-09-23 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2019-09-12 to 2019-09-23 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2018-04-01 to 2018-04-11 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2018-04-01 to 2018-04-11 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2018-04-01 to 2018-04-11 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2018-09-12 to 2018-09-23 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2018-09-12 to 2018-09-23 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2018-09-12 to 2018-09-23 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2017-04-01 to 2017-04-11 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2017-04-01 to 2017-04-11 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2017-04-01 to 2017-04-11 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2017-09-12 to 2017-09-23 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2017-09-12 to 2017-09-23 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2017-09-12 to 2017-09-23 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2016-04-01 to 2016-04-11 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2016-04-01 to 2016-04-11 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2016-04-01 to 2016-04-11 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2016-09-12 to 2016-09-23 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2016-09-12 to 2016-09-23 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2016-09-12 to 2016-09-23 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2015-04-01 to 2015-04-11 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2015-04-01 to 2015-04-11 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2015-04-01 to 2015-04-11 (max of 60)\n",
      "Downloading tweets for query: '#ncaa' from 2015-09-12 to 2015-09-23 (max of 60)\n",
      "Downloading tweets for query: 'ncaa' from 2015-09-12 to 2015-09-23 (max of 60)\n",
      "Downloading tweets for query: '#title9' from 2015-09-12 to 2015-09-23 (max of 60)\n"
     ]
    }
   ],
   "source": [
    "dates = [(\"2020-04-01\",'2020-04-11'),\n",
    "         ('2020-09-12','2020-09-23'),\n",
    "         ('2019-04-01','2019-04-11'),\n",
    "         ('2019-09-12','2019-09-23'),\n",
    "         ('2018-04-01','2018-04-11'),\n",
    "         ('2018-09-12','2018-09-23'),\n",
    "         ('2017-04-01','2017-04-11'),\n",
    "         ('2017-09-12','2017-09-23'),\n",
    "         ('2016-04-01','2016-04-11'),\n",
    "         ('2016-09-12','2016-09-23'),\n",
    "         ('2015-04-01','2015-04-11'),\n",
    "         ('2015-09-12','2015-09-23')\n",
    "        ]\n",
    "\n",
    "queries = ['#ncaa','ncaa','#title9']\n",
    "\n",
    "for daterange in dates:\n",
    "    ranged_query(daterange,queries,'data',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
